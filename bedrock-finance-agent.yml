AWSTemplateFormatVersion: 2010-09-09
Description: >-
  Amazon Bedrock Agents with Observability Tutorial
Parameters:
  EnvironmentTag:
    Description: Enter Environment Tag
    Type: String
    Default: 'dev'
  CIDRPrefix:
    Description: 'Enter Class B CIDR Prefix (e.g. 192.168, 10.1, 172.16)'
    Type: String
    AllowedPattern: '(192\.168)|10\.[0-9][0-9]{0,1}|(172\.([1][6-9]|[2][0-9]|[3][0-1]))'
    ConstraintDescription: >-
      must be a valid Private Subnet CIDR Prefix between 192.168 or 10.{0-99} or
      172.16
    Default: '192.168'
  DomainName:
    Description: User-defined OpenSearch domain name
    Type: String
    Default: 'aos'
  ECRRepoName:
    Description: ECR repo name
    Type: String
    Default: 'financetools'
  InstanceType:
    Description: OpenSearchService EC2 instance type
    Type: String
    Default: 'r6g.large.search'
  EEKeyPair:
    Description: Amazon EC2 Key Pair
    Type: 'AWS::EC2::KeyPair::KeyName'
    Default: 'felixh'
    MinLength: 1
  LatestAmiId:
    Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2
    AllowedValues:
      - /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2
    Description: Image ID for the EC2 helper instance. DO NOT change this.
  AgentFoundationModel:
    Description: Amazon Bedrock Agent Foundation Model
    Type: String
    Default: 'anthropic.claude-instant-v1'
  AgentName:
    Description: Amazon Bedrock Agent Name
    Type: String
    Default: 'financetstagent'
  AgentInstruction:
    Description: Amazon Bedrock Agent Instruction
    Type: String
    Default: 'Agent Finance is an automated, AI-powered agent that helps customers with financial investments.'
  AgentActionGroupName:
    Description: Amazon Bedrock Agent ActionGroupName
    Type: String
    Default: 'financetstgrp'
  KnowledgeBaseName:
    Description: Amazon Bedrock KnowledgeBase Name
    Type: String
    Default: 'financetstkb'
  KnowledgeBaseDescription:
    Description: Amazon Bedrock KnowledgeBase Description
    Type: String
    Default: 'This knowledge base contains financial information.'    
Resources:
  VPC:
    Type: 'AWS::EC2::VPC'
    Properties:
      CidrBlock: !Join 
        - ''
        - - !Ref CIDRPrefix
          - .0.0/21
      EnableDnsHostnames: 'true'
      EnableDnsSupport: 'true'
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-vpc'
  InternetGateway:
    Type: 'AWS::EC2::InternetGateway'
    Properties:
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-igw'
  AttachInternetGateway:
    Type: 'AWS::EC2::VPCGatewayAttachment'
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway
  PublicSubnet0:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select 
        - '0'
        - !GetAZs ''
      CidrBlock: !Join 
        - ''
        - - !Ref CIDRPrefix
          - .0.0/24
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-sn-pub0'
  PublicSubnet1:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select 
        - '1'
        - !GetAZs ''
      CidrBlock: !Join 
        - ''
        - - !Ref CIDRPrefix
          - .1.0/24
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-sn-pub1'
  PublicSubnet2:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select 
        - '2'
        - !GetAZs ''
      CidrBlock: !Join 
        - ''
        - - !Ref CIDRPrefix
          - .2.0/24
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-sn-pub2'
  PrivateSubnetApp0:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select 
        - '0'
        - !GetAZs ''
      CidrBlock: !Join 
        - ''
        - - !Ref CIDRPrefix
          - .4.0/24
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-sn-priv-app0'
  PrivateSubnetApp1:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select 
        - '1'
        - !GetAZs ''
      CidrBlock: !Join 
        - ''
        - - !Ref CIDRPrefix
          - .5.0/24
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-sn-priv-app1'
  PrivateSubnetApp2:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select 
        - '2'
        - !GetAZs ''
      CidrBlock: !Join 
        - ''
        - - !Ref CIDRPrefix
          - .6.0/24
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-sn-priv-app2'
  PublicRoutingTable:
    Type: 'AWS::EC2::RouteTable'
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-rtbl-pub'
  PublicRoute:
    Type: 'AWS::EC2::Route'
    Properties:
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway
      RouteTableId: !Ref PublicRoutingTable
  PublicRouteAssociation0:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      RouteTableId: !Ref PublicRoutingTable
      SubnetId: !Ref PublicSubnet0
  PublicRouteAssociation1:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      RouteTableId: !Ref PublicRoutingTable
      SubnetId: !Ref PublicSubnet1
  PublicRouteAssociation2:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      RouteTableId: !Ref PublicRoutingTable
      SubnetId: !Ref PublicSubnet2
  PrivateRoutingTable:
    Type: 'AWS::EC2::RouteTable'
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Join 
            - ''
            - - !Ref EnvironmentTag
              - '-rtbl-priv'
  NATGatewayIPAddress:
    Type: 'AWS::EC2::EIP'
    DependsOn: AttachInternetGateway
    Properties:
      Domain: vpc
  NATGateway:
    Type: 'AWS::EC2::NatGateway'
    Properties:
      AllocationId: !GetAtt 
        - NATGatewayIPAddress
        - AllocationId
      SubnetId: !Ref PublicSubnet0
  PrivateRoute:
    Type: 'AWS::EC2::Route'
    Properties:
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId: !Ref NATGateway
      RouteTableId: !Ref PrivateRoutingTable
  PrivateRouteAssociationApp0:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      RouteTableId: !Ref PrivateRoutingTable
      SubnetId: !Ref PrivateSubnetApp0
  PrivateRouteAssociationApp1:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      RouteTableId: !Ref PrivateRoutingTable
      SubnetId: !Ref PrivateSubnetApp1
  PrivateRouteAssociationApp2:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      RouteTableId: !Ref PrivateRoutingTable
      SubnetId: !Ref PrivateSubnetApp2


  OpenSearchServiceDomain:
    Type: 'AWS::OpenSearchService::Domain'
    Properties:
      DomainName: !Join ['', [!Ref DomainName, !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]
      EngineVersion: OpenSearch_2.11
      ClusterConfig:
        InstanceCount: 1
        InstanceType: !Ref InstanceType
      EBSOptions:
        EBSEnabled: 'true'
        Iops: '3000'
        VolumeSize: 100
        VolumeType: 'gp3'
      AdvancedOptions:
        rest.action.multi.allow_explicit_index: 'true'
        override_main_response_version: 'true'      
      AccessPolicies:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              AWS: "*"
            Action:
              - 'es:*'
            Resource: !Join 
              - ''
              - - 'arn:aws:es:'
                - !Ref 'AWS::Region'
                - ':'
                - !Ref 'AWS::AccountId'
                - ':domain/'
                #- !Join ['', [!Ref DomainName, !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]
                - '*'
      VPCOptions:
        SubnetIds:
          - !Ref PrivateSubnetApp1
        SecurityGroupIds:
          - !Ref opsSecurityGroup

  opsSecurityGroup:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Security group for OpenSearch service
      VpcId: !Ref VPC
      GroupName: !Sub 
        - 'secgr-${SgName}'
        - SgName: !Ref DomainName
      SecurityGroupIngress:
        - FromPort: 443
          IpProtocol: tcp
          ToPort: 443
          CidrIp: 0.0.0.0/0


  DataBucket:
    Type: AWS::S3::Bucket
    Properties:
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True

# ecr repository
  AgentToolsRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Ref ECRRepoName
      ImageScanningConfiguration:
        ScanOnPush: true
      ImageTagMutability: MUTABLE

# resource to push docker image to ECR
  EC2InstanceProfile:
    Type: 'AWS::IAM::InstanceProfile'
    Properties:
      Path: /
      Roles:
        - !Ref BedrockAgentToolsFunctionRole

  DockerPushInstance:
    Type: 'AWS::EC2::Instance'
    DependsOn:
      - BedrockAgentToolsFunctionRole
    Metadata:  
      AWS::CloudFormation::Init:
            configSets:
                ec2_bootstrap:
                    - install_docker
            install_docker:
                packages:
                    yum:
                        docker: []
                services:
                    sysvinit:
                        docker:
                            enabled: "true"
                            ensureRunning: "true"
                commands:
                    docker_for_ec2_user:
                        command: usermod -G docker ec2-user
    Properties:
      InstanceType: t2.small
      ImageId: !Ref LatestAmiId
      KeyName: !Ref EEKeyPair
      IamInstanceProfile: !Ref EC2InstanceProfile
      NetworkInterfaces:
        - AssociatePublicIpAddress: 'true'
          DeviceIndex: '0'
          GroupSet:
            - !Ref opsSecurityGroup
          SubnetId: !Ref PublicSubnet1
      UserData: !Base64
        'Fn::Sub':
          |
            #!/bin/bash -xe
            # sudo yum update -y && sudo amazon-linux-extras install docker -y && sudo service docker start && sudo usermod -a -G docker ec2-user
            su - ec2-user
            sudo yum update -y aws-cfn-bootstrap
            # Start cfn-init
            /opt/aws/bin/cfn-init -s ${AWS::StackId} -r DockerPushInstance --configsets ec2_bootstrap --region ${AWS::Region}
            
            sudo docker pull felix85/bedrock_tools
            sudo docker tag felix85/bedrock_tools:latest ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepoName}
            aws ecr get-login-password --region ${AWS::Region} | sudo docker login --username AWS --password-stdin ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com
            sudo docker push ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepoName}
            # All done so signal success
            /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackId} --resource DockerPushInstance --region ${AWS::Region}     
      Tags:
        - Key: Name
          Value: ECR-Loader

  OpenSearchIngestionTrace:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Join ['', ['/aws/vendedlogs/', !Join ['', ['trace-pipeline', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]]]
      RetentionInDays: 1

  OpenSearchIngestionTracePipeline:
    Type: 'AWS::OSIS::Pipeline'
    DependsOn:
      - BedrockAgentToolsFunctionRole
      - PrivateSubnetApp1
      - opsSecurityGroup
    Properties:
      LogPublishingOptions:
        IsLoggingEnabled: true
        CloudWatchLogDestination:
          LogGroup: !Join ['', ['/aws/vendedlogs/', !Join ['', ['trace-pipeline', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]]]
      MinUnits: 3
      MaxUnits: 9
      PipelineName: !Join ['', ['trace-pipeline', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]
      VpcOptions:
        SubnetIds:
          - !Ref PrivateSubnetApp1
        SecurityGroupIds:
          - !Ref opsSecurityGroup
      PipelineConfigurationBody:
        !Sub |
        ###
          # Limitations: https://docs.aws.amazon.com/opensearch-service/latest/ingestion/ingestion.html#ingestion-limitations
          ## Relevant Documentation for this blueprint
            # https://docs.aws.amazon.com/opensearch-service/latest/developerguide/use-cases-trace-analytics.html
            # https://docs.aws.amazon.com/opensearch-service/latest/developerguide/pipeline-domain-access.html
            # https://docs.aws.amazon.com/opensearch-service/latest/developerguide/configure-client.html#configure-client-otel
          ##
        ###
        ###
          ## entry-pipeline:
            # This pipeline receives trace data from OpenTelemetry collector, and forwards the trace data
            # to both the 'span-pipeline' and the 'service-map-pipeline'
          ##
          ## span-pipeline:
            # This pipeline receives the trace data from the 'entry-pipeline`, and then extracts all of the raw spans
            # from the traces. The raw spans are then sent to OpenSearch to an index that is prefixed with 'otel-v1-apm-span'.
          ##
          ## service-map-pipeline:
            # This pipeline receives the trace data from the `entry-pipeline', and then aggregates and analyzes the trace data to create documents
            # that represent connections between services. These service map documents are then sent to OpenSearch to an index named 'otel-v1-apm-service-map',
            # which allows one to see a visualization of the service map through the Trace Analytics plugin in OpenSearch Dashboards.
          ##
        ###

        version: "2"
        entry-pipeline:
          source:
            otel_trace_source:
              path: "/v1/traces"
          processor:
            - trace_peer_forwarder:
          sink:
            - pipeline:
                name: "span-pipeline"
            - pipeline:
                name: "service-map-pipeline"
        span-pipeline:
          source:
            pipeline:
              name: "entry-pipeline"
          processor:
            - otel_traces:
              # For reference please visit: https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/processors/otel-trace-raw/
          sink:
            - opensearch:
                # Provide an AWS OpenSearch Service domain endpoint
                hosts: [ "https://${OpenSearchServiceDomain.DomainEndpoint}" ]
                aws:
                  # Provide a Role ARN with access to the domain. This role should have a trust relationship with osis-pipelines.amazonaws.com
                  sts_role_arn: "${BedrockAgentToolsFunctionRole.Arn}"
                  # Provide the region of the domain.
                  region: "${AWS::Region}"
                  # Enable the 'serverless' flag if the sink is an Amazon OpenSearch Serverless collection
                  # serverless: true
                  # serverless_options:
                    # Specify a name here to create or update network policy for the serverless collection
                    # network_policy_name: "network-policy-name"
                index_type: "trace-analytics-raw"
                # Enable the S3 DLQ to capture any failed requests in an S3 bucket
                # dlq:
                  # s3:
                    # Provide an S3 bucket
                    # bucket: "your-dlq-bucket-name"
                    # Provide a key path prefix for the failed requests
                    # key_path_prefix: "span-pipeline/trace-analytics/dlq/"
                    # Provide the region of the bucket.
                    # region: "${AWS::Region}"
                    # Provide a Role ARN with access to the bucket. This role should have a trust relationship with osis-pipelines.amazonaws.com
                    # sts_role_arn: "${BedrockAgentToolsFunctionRole.Arn}"
        service-map-pipeline:
          source:
            pipeline:
              name: "entry-pipeline"
          processor:
            - service_map:
              # For reference please visit: https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/processors/service-map-stateful/
          sink:
            - opensearch:
                # Provide an AWS OpenSearch Service domain endpoint
                hosts: [ "https://${OpenSearchServiceDomain.DomainEndpoint}" ]
                aws:
                  # Provide a Role ARN with access to the domain. This role should have a trust relationship with osis-pipelines.amazonaws.com
                  sts_role_arn: "${BedrockAgentToolsFunctionRole.Arn}"
                  # Provide the region of the domain.
                  region: "${AWS::Region}"
                  # Enable the 'serverless' flag if the sink is an Amazon OpenSearch Serverless collection
                  # serverless: true
                  # serverless_options:
                    # Specify a name here to create or update network policy for the serverless collection
                    # network_policy_name: "network-policy-name"
                index_type: "trace-analytics-service-map"
                # Enable the S3 DLQ to capture any failed requests in an S3 bucket
                # dlq:
                  # s3:
                    # Provide an S3 bucket
                    # bucket: "your-dlq-bucket-name"
                    # Provide a key path prefix for the failed requests
                    # key_path_prefix: "service-map-pipeline/trace-analytics-service-map/dlq"
                    # Provide the region of the bucket.
                    # region: "${AWS::Region}"
                    # Provide a Role ARN with access to the bucket. This role should have a trust relationship with osis-pipelines.amazonaws.com
                    # sts_role_arn: "${BedrockAgentToolsFunctionRole.Arn}"


  OpenSearchIngestionMetric:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Join ['', ['/aws/vendedlogs/', !Join ['', ['metric-pipeline', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]]]
      RetentionInDays: 1

  OpenSearchIngestionMetricPipeline:
    Type: 'AWS::OSIS::Pipeline'
    DependsOn:
      - BedrockAgentToolsFunctionRole
      - PrivateSubnetApp1
      - opsSecurityGroup
    Properties:
      LogPublishingOptions:
        IsLoggingEnabled: true
        CloudWatchLogDestination:
          LogGroup: !Join ['', ['/aws/vendedlogs/', !Join ['', ['metric-pipeline', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]]]
      MinUnits: 3
      MaxUnits: 9
      PipelineName: !Join ['', ['metric-pipeline', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]
      VpcOptions:
        SubnetIds:
          - !Ref PrivateSubnetApp1
        SecurityGroupIds:
          - !Ref opsSecurityGroup
      PipelineConfigurationBody:
        !Sub |
          ###
            # Limitations: https://docs.aws.amazon.com/opensearch-service/latest/ingestion/ingestion.html#ingestion-limitations
            ## Relevant Documentation for this blueprint
              # https://docs.aws.amazon.com/opensearch-service/latest/developerguide/use-cases-overview.html
              # https://docs.aws.amazon.com/opensearch-service/latest/developerguide/configure-client.html#configure-client-otel
              # https://docs.aws.amazon.com/opensearch-service/latest/developerguide/pipeline-domain-access.html
            ##
          ###
          ###
            # otel-metrics-pipeline:
              # This pipeline receives metrics from OpenTelemetry Collector, converts the ExportMetricsServiceRequest to String values, and writes
              # the result to OpenSearch to an index named 'metrics'
          ###

          version: "2"
          otel-metrics-pipeline:
            source:
              otel_metrics_source:
                path: "/v1/metrics"
            processor:
              # For more configuration parameters, see https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/processors/otel-metrics/
              - otel_metrics:
            sink:
              - opensearch:
                  # Provide an AWS OpenSearch Service domain endpoint
                  hosts: [ "https://${OpenSearchServiceDomain.DomainEndpoint}" ]
                  aws:
                    # Provide a Role ARN with access to the domain. This role should have a trust relationship with osis-pipelines.amazonaws.com
                    sts_role_arn: "${BedrockAgentToolsFunctionRole.Arn}"
                    # Provide the region of the domain.
                    region: "${AWS::Region}"
                    # Enable the 'serverless' flag if the sink is an Amazon OpenSearch Serverless collection
                    # serverless: true
                    # serverless_options:
                      # Specify a name here to create or update network policy for the serverless collection
                      # network_policy_name: "network-policy-name"
                  index: "metrics"
                  # Enable the 'distribution_version' setting if the AWS OpenSearch Service domain is of version Elasticsearch 6.x
                  # distribution_version: "es6"
                  # Enable and switch the 'enable_request_compression' flag if the default compression setting is changed in the domain. See https://docs.aws.amazon.com/opensearch-service/latest/developerguide/gzip.html
                  # enable_request_compression: true/false
                  # Enable the S3 DLQ to capture any failed requests in an S3 bucket
                  # dlq:
                    # s3:
                      # Provide an S3 bucket
                      # bucket: "your-dlq-bucket-name"
                      # Provide a key path prefix for the failed requests
                      # key_path_prefix: "apache-log-pipeline/logs/dlq"
                      # Provide the region of the bucket.
                      # region: "${AWS::Region}"
                      # Provide a Role ARN with access to the bucket. This role should have a trust relationship with osis-pipelines.amazonaws.com
                      # sts_role_arn: "${BedrockAgentToolsFunctionRole.Arn}"

  OpenSearchIngestionLog:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Join ['', ['/aws/vendedlogs/', !Join ['', ['log-pipeline', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]]]
      RetentionInDays: 1

  OpenSearchIngestionLogPipeline:
    Type: 'AWS::OSIS::Pipeline'
    DependsOn:
      - BedrockAgentToolsFunctionRole
      - PrivateSubnetApp1
      - opsSecurityGroup
    Properties:
      LogPublishingOptions:
        IsLoggingEnabled: true
        CloudWatchLogDestination:
          LogGroup: !Join ['', ['/aws/vendedlogs/', !Join ['', ['log-pipeline', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]]]
      MinUnits: 3
      MaxUnits: 9
      PipelineName: !Join ['', ['log-pipeline', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]
      VpcOptions:
        SubnetIds:
          - !Ref PrivateSubnetApp1
        SecurityGroupIds:
          - !Ref opsSecurityGroup
      PipelineConfigurationBody:
        !Sub |    
            ###
              # Limitations: https://docs.aws.amazon.com/opensearch-service/latest/ingestion/ingestion.html#ingestion-limitations
              ## Relevant Documentation for this blueprint
                # https://docs.aws.amazon.com/opensearch-service/latest/developerguide/use-cases-overview.html
                # https://docs.aws.amazon.com/opensearch-service/latest/developerguide/configure-client.html#configure-client-otel
                # https://docs.aws.amazon.com/opensearch-service/latest/developerguide/pipeline-domain-access.html
              ##
            ###
            ###
              # otel-logs-pipeline:
              # This pipeline receives metrics from OpenTelemetry Collector and writes
              # the result to OpenSearch to an index named 'logs'
            ###

            version: "2"
            otel-logs-pipeline:
              source:
                otel_logs_source:
                  path: "/v1/logs"
              sink:
                - opensearch:
                    # Provide an AWS OpenSearch Service domain endpoint
                    hosts: [ "https://${OpenSearchServiceDomain.DomainEndpoint}" ]
                    aws:
                      # Provide a Role ARN with access to the domain. This role should have a trust relationship with osis-pipelines.amazonaws.com
                      sts_role_arn: "${BedrockAgentToolsFunctionRole.Arn}"
                      # Provide the region of the domain.
                      region: "${AWS::Region}"
                      # Enable the 'serverless' flag if the sink is an Amazon OpenSearch Serverless collection
                      # serverless: true
                      # serverless_options:
                        # Specify a name here to create or update network policy for the serverless collection
                        # network_policy_name: "network-policy-name"
                    index: "logs"
                    # Enable the 'distribution_version' setting if the AWS OpenSearch Service domain is of version Elasticsearch 6.x
                    # distribution_version: "es6"
                    # Enable and switch the 'enable_request_compression' flag if the default compression setting is changed in the domain. See https://docs.aws.amazon.com/opensearch-service/latest/developerguide/gzip.html
                    # enable_request_compression: true/false
                    # Enable the S3 DLQ to capture any failed requests in an S3 bucket
                    # dlq:
                      # s3:
                        # Provide an S3 bucket
                        # bucket: "your-dlq-bucket-name"
                        # Provide a key path prefix for the failed requests
                        # key_path_prefix: "apache-log-pipeline/logs/dlq"
                        # Provide the region of the bucket.
                        # region: "${AWS::Region}"
                        # Provide a Role ARN with access to the bucket. This role should have a trust relationship with osis-pipelines.amazonaws.com
                        # sts_role_arn: "${BedrockAgentToolsFunctionRole.Arn}"  

# lambda function for Bedrock Agent
  BedrockAgentToolsLambdaFunction:
    Type: AWS::Lambda::Function
    DependsOn: 
      - DockerPushInstance
    CreationPolicy:
      ResourceSignal:
        Timeout: PT15M
    Properties:
      Code:
        ImageUri: !Sub '${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepoName}:latest'
      PackageType: Image
      Role: !GetAtt BedrockAgentToolsFunctionRole.Arn
      VpcConfig:
          SecurityGroupIds: 
            - !Ref opsSecurityGroup
          SubnetIds:
            - !Ref PrivateSubnetApp1
      Environment:
        Variables:
          AWS_LAMBDA_EXEC_WRAPPER: '/opt/otel-instrument'
          OPENTELEMETRY_COLLECTOR_CONFIG_FILE: '/var/task/collector.yml'
          OTEL_SERVICE_NAME: 'FinancialAgent'
          LOGS_ENDPOINT: !Join ['', ['https://', !Select [0, !GetAtt OpenSearchIngestionLogPipeline.IngestEndpointUrls],'/v1/logs']]
          METRICS_ENDPOINT: !Join ['', ['https://', !Select [0, !GetAtt OpenSearchIngestionMetricPipeline.IngestEndpointUrls],'/v1/metrics']]
          TRACE_ENDPOINT: !Join ['', ['https://', !Select [0, !GetAtt OpenSearchIngestionTracePipeline.IngestEndpointUrls],'/v1/traces']]
      Timeout: 900
      EphemeralStorage: 
        Size: 1028
      MemorySize: '1028'


  LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref BedrockAgentToolsLambdaFunction
      Action: lambda:InvokeFunction
      Principal: "bedrock.amazonaws.com"  


  BedrockKBRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Join ['_', ['AmazonBedrockExecutionRoleForKnowledgeBase_', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - bedrock.amazonaws.com
                - opensearchservice.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AmazonS3FullAccess'
        - 'arn:aws:iam::aws:policy/SecretsManagerReadWrite'
        - 'arn:aws:iam::aws:policy/AmazonBedrockFullAccess'
      Policies:
        - PolicyName: CloudWatchAccess
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: "*"
        - PolicyName: opensearch
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
            - Effect: Allow
              Action:
              - 'aoss:*'
              - 'aoss:APIAccessAll'
              Resource:
              - !Sub '${TestCollection.Arn}*'
        - PolicyName: s3access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
            - Effect: Allow
              Action:
              - 's3:Get*'
              - 's3:List*'
              - 's3:AbortMultipartUpload'
              - 's3:DeleteObject'
              - 's3:GetBucketVersioning'
              - 's3:GetObject'
              - 's3:GetObjectTagging'
              - 's3:GetObjectVersion'
              - 's3:ListBucket'
              - 's3:ListBucketMultipartUploads'
              - 's3:ListBucketVersions'
              - 's3:ListMultipartUploadParts'
              - 's3:PutBucketVersioning'
              - 's3:PutObject'
              - 's3:PutObjectTagging'
              Resource:
              - !Sub '${DataBucket.Arn}/*'
              - !Sub '${DataBucket.Arn}'
  

  KBPassAccessPolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      PolicyDocument: !Sub |
          {
              "Version": "2012-10-17",
              "Statement": [
                  {
                      "Effect": "Allow",
                      "Action": "iam:PassRole",
                      "Resource": "arn:aws:iam::${AWS::AccountId}:role/${BedrockKBRole}"
                  }
              ]
          }      
      Roles: 
        - !Ref   BedrockKBRole
      PolicyName: !Join ['_', ['kb_iam_pass_policy', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]


  BedrockAgentToolsFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Join ['_', ['AmazonBedrockExecutionRoleForAgents', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
                - bedrock.amazonaws.com
                - athena.amazonaws.com
                - opensearchservice.amazonaws.com
                - es.amazonaws.com
                - osis.amazonaws.com
                - osis-pipelines.amazonaws.com
                - ec2.amazonaws.com
                - glue.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AmazonS3FullAccess'
        - 'arn:aws:iam::aws:policy/AWSLambda_FullAccess'
        - 'arn:aws:iam::aws:policy/AmazonBedrockFullAccess'
        - 'arn:aws:iam::aws:policy/AmazonEC2FullAccess'
        - 'arn:aws:iam::aws:policy/SecretsManagerReadWrite'
        - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'

      Policies:
        - PolicyName: ECRGetAuthorizationToken
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ecr:*
                Resource: "*"
        - PolicyName: GlueAthenaBedrockAccess
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:*
                  - glue:GetTables
                  - glue:GetTable
                  - glue:CreateDatabase
                  - glue:DeleteDatabase
                  - glue:DeleteCrawler
                  - glue:CreateCrawler
                  - glue:StartCrawler
                  - athena:GetWorkGroup
                  - athena:StartQueryExecution
                  - athena:CancelQueryExecution
                  - athena:StopQueryExecution
                  - athena:GetQueryExecution
                  - athena:GetQueryResults
                  - athena:ListDataCatalogs
                  - athena:ListWorkGroups
                  - athena:UpdateWorkGroup
                Resource: "*"
        - PolicyName: OpenSearchIngestionAccess
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - osis:Ingest
                  - 'es:describeDomain'
                  - 'es:ESHttpGet'
                  - 'es:ESHttpPut'
                Resource: "*"
        - PolicyName: CloudWatchAccess
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: "*"
        - PolicyName: opensearch
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
            - Effect: Allow
              Action:
              - 'aoss:*'
              Resource:
              - !Sub '${TestCollection.Arn}*'
        - PolicyName: s3access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
            - Effect: Allow
              Action:
              - 's3:Get*'
              - 's3:List*'
              - 's3:AbortMultipartUpload'
              - 's3:DeleteObject'
              - 's3:GetBucketVersioning'
              - 's3:GetObject'
              - 's3:GetObjectTagging'
              - 's3:GetObjectVersion'
              - 's3:ListBucket'
              - 's3:ListBucketMultipartUploads'
              - 's3:ListBucketVersions'
              - 's3:ListMultipartUploadParts'
              - 's3:PutBucketVersioning'
              - 's3:PutObject'
              - 's3:PutObjectTagging'
              Resource:
              - !Sub '${DataBucket.Arn}/*'
              - !Sub '${DataBucket.Arn}'


  IAMPassAccessPolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      PolicyDocument: !Sub |
          {
              "Version": "2012-10-17",
              "Statement": [
                  {
                      "Effect": "Allow",
                      "Action": "iam:PassRole",
                      "Resource": "arn:aws:iam::${AWS::AccountId}:role/${BedrockAgentToolsFunctionRole}"
                  }
              ]
          }
            
      Roles: 
        - !Ref   BedrockAgentToolsFunctionRole
      PolicyName: !Join ['_', ['agent_iam_pass_policy', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]


  TestCollection:
    Type: 'AWS::OpenSearchServerless::Collection'
    DependsOn: 
      - EncryptionPolicy
    Properties:
      Name: !Sub vector${AWS::StackName}
      Type: VECTORSEARCH
      Description: Opensearch Vector Collection for FinOps Chat
    

  EncryptionPolicy:
    Type: 'AWS::OpenSearchServerless::SecurityPolicy'
    Properties:
      Name: !Sub ${AWS::StackName}-encrypt
      Type: encryption
      Description: !Sub Encryption policy for vector-${AWS::StackName}
      Policy: !Sub >-
        {"Rules":[{"ResourceType":"collection","Resource":["collection/vector${AWS::StackName}"]}],"AWSOwnedKey":true}
  
  NetworkPolicy:
    Type: 'AWS::OpenSearchServerless::SecurityPolicy'
    Properties:
      Name: !Sub ${AWS::StackName}-security
      Type: network
      Description: !Sub Security policy for vector-${AWS::StackName}
      Policy: !Sub >-
        [{"Rules":[{"ResourceType":"collection","Resource":["collection/vector${AWS::StackName}"]}, {"ResourceType":"dashboard","Resource":["collection/vector${AWS::StackName}"]}],"AllowFromPublic":true}]
  
  TestAccessPolicy:
    Type: 'AWS::OpenSearchServerless::AccessPolicy'
    Properties:
      Name: !Sub ${AWS::StackName}-access
      Type: data
      Description: !Sub Access policy for vector-${AWS::StackName}
      Policy: !Sub >-
        [{"Description":"Access for test-user","Rules":[{"ResourceType":"index","Resource":["index/*/*"],"Permission":["aoss:*"]},
        {"ResourceType":"collection","Resource":["collection/vector${AWS::StackName}"],"Permission":["aoss:*"]}],
        "Principal":["${BedrockAgentToolsFunctionRole.Arn}", "${BedrockKBRole.Arn}"]}]


  LambdaSetupFunction:
    Type: AWS::Lambda::Function
    DependsOn: 
      - BedrockAgentToolsFunctionRole
      - DataBucket
    Properties:
        Description: ""
        FunctionName: !Join ['_', ['setup_data_lambda', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]
        Handler: "index.lambda_handler"
        Code: 
            ZipFile : |
              import json
              import boto3
              import os
              import cfnresponse
              import string
              import random
              import urllib3
              import shutil
              import time
              from botocore.exceptions import ClientError

              def wait_for_crawler_creation(crawler_name):
                  glue_client = boto3.client('glue')

                  waiter = glue_client.get_waiter('crawler_exists')
                  waiter.wait(
                      CrawlerNameList=[crawler_name],
                      WaiterConfig={
                          'Delay': 10,
                          'MaxAttempts': 30
                      }
                  )

              def run_glue_crawler(crawler_name):
                  glue_client = boto3.client('glue')

                  response = glue_client.start_crawler(
                      Name=crawler_name
                  )

                  return response
                  
              def download_public_files(src,tgt):
                  http = urllib3.PoolManager()
                  with open("/tmp/"+tgt, 'wb') as out:
                    r = http.request('GET', src, preload_content=False)
                    shutil.copyfileobj(r, out)
                  return "Files Downloaded Locally"

              def empty_bucket(bucket_name,region_name,account_id):
                  try:
                    s3 = boto3.resource('s3')
                    bucket = s3.Bucket(bucket_name)
                    bucket.objects.all().delete()  
                    
                  except Exception as e:
                    print(str(e))
                  return "Bucket {} Emptied ".format(bucket_name) 

              def drop_database(GlueDatabaseName, pattern):
                  try:
                    glue = boto3.client('glue')
                    #db_name = GlueDatabaseName+pattern
                    db_name = 'investment_portfolio'
                    glue.delete_database(Name=db_name) 
                  except Exception as e:
                    print(str(e))
                  return "DB {} Deleted ".format(db_name)  

              def delete_crawler(pattern):
                  crawler_name = "stockportfolio-"+pattern
                  try:
                    glue = boto3.client('glue')
                    glue.delete_crawler(Name=crawler_name)
                  except Exception as e:
                    print(str(e))
                  return "Crawler {} deleted ".format(crawler_name) 

              def configure_athena_result_location(bucket_name):
                  print('start athena result location configuration')
                  client = boto3.client('athena')
                  try:
                      response = client.get_work_group(WorkGroup='primary')
                      ConfigurationUpdates={}
                      ConfigurationUpdates['EnforceWorkGroupConfiguration']= True
                      ResultConfigurationUpdates= {}
                      athena_location = "s3://"+ bucket_name +"/athena_results/"
                      ResultConfigurationUpdates['OutputLocation']=athena_location
                      EngineVersion = response['WorkGroup']['Configuration']['EngineVersion']
                      ConfigurationUpdates['ResultConfigurationUpdates']=ResultConfigurationUpdates
                      ConfigurationUpdates['PublishCloudWatchMetricsEnabled']= response['WorkGroup']['Configuration']['PublishCloudWatchMetricsEnabled']
                      ConfigurationUpdates['EngineVersion']=EngineVersion
                      ConfigurationUpdates['RequesterPaysEnabled']= response['WorkGroup']['Configuration']['RequesterPaysEnabled']
                      response2 = client.update_work_group(WorkGroup='primary',ConfigurationUpdates=ConfigurationUpdates,State='ENABLED')
                  except Exception as e:
                    print(str(e))
                  
                  print("athena output location updated")
                  return "athena output location updated to s3://{}/athena_results/".format(bucket_name)                                 

              def provision_s3_dirs(bucket_name,region_name,account_id,ret_dict):
                  print("BUCKET NAME IS "+bucket_name)
                  
                  s3 = boto3.client('s3')
                  try:
                    s3.put_object(Bucket=bucket_name, Key=("code/"))
                    s3.put_object(Bucket=bucket_name, Key=("athena_results/"))
                    s3.put_object(Bucket=bucket_name, Key=("data/"))
                    s3.put_object(Bucket=bucket_name, Key=("kb/"))

                  except Exception as e:
                    print(str(e))
                  
                  try:
                      assets3 = boto3.resource('s3')
                      if assets3.Bucket(bucket_name).creation_date is None:
                        if region_name == 'us-east-1':
                            print('trying to create bucket')
                            assets3.create_bucket(Bucket=bucket_name)
                        else:
                            print('other region')
                            assets3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration={'LocationConstraint':region_name})
                        print("Asset bucket {} doesn't exists, created".format(bucket_name))
                        time.sleep(20)
                        print("End Timed wait after Asset bucket created")
                  except Exception as e:
                      print(str(e))
                  
                  configure_athena_result_location(bucket_name)
                  
                  ret_dict["WorkshopBucket"]=bucket_name
                  return ret_dict

              def deploy_assets(bucket_name,region_name,account_id,role_arn, ret_dict):
                  print("deploy assets to bucket: "+bucket_name)
                  try :
                      download_public_files("https://d3q8adh3y5sxpk.cloudfront.net/tutorial/stock_portfolio.csv","stock_portfolio.csv")
                      download_public_files("https://d3q8adh3y5sxpk.cloudfront.net/tutorial/amazon_10k_2023.pdf","amazon_10k_2023.pdf")
                      download_public_files("https://d3q8adh3y5sxpk.cloudfront.net/tutorial/agent_aws_openapi.json","agent_aws_openapi.json")
                      download_public_files("https://d3q8adh3y5sxpk.cloudfront.net/tutorial/opensearch-lib.zip","opensearch-lib.zip")
                      
                      s3_client = boto3.client('s3')
                      s3_client.upload_file('/tmp/stock_portfolio.csv', bucket_name, 'data/stock_portfolio/stock_portfolio.csv')
                      s3_client.upload_file('/tmp/amazon_10k_2023.pdf', bucket_name, 'kb/amazon_10k_2023.pdf')
                      s3_client.upload_file('/tmp/agent_aws_openapi.json', bucket_name, 'code/agent_aws_openapi.json')
                      s3_client.upload_file('/tmp/opensearch-lib.zip', bucket_name, 'code/opensearch-lib.zip')
                      

                  except Exception as e:
                      print("Failed provisioning assets "+str(e))
                  
                  return ret_dict

              def create_glue_database(bucket_name,GlueDatabaseName, pattern,ret_dict):
                  print("create Glue database: "+GlueDatabaseName)
                  try :
                      glue = boto3.client('glue')
                      #db_name = GlueDatabaseName+pattern
                      db_name = 'investment_portfolio'
                      db_location = "s3://"+ bucket_name +"/database_path/glue-etl-redshift/"
                      glue.create_database(DatabaseInput={'Name': db_name,'LocationUri': db_location})
                  except Exception as e:
                      print("Failed provisioning glue database "+str(e))
                  ret_dict["DatabaseName"]=db_name
                  return ret_dict

              
              def deploy_and_run_glue_crawler(role_arn, pattern, GlueDatabaseName, bucket_name, ret_dict):
                  print("start Glue crawler creation")
                  crawler_name = "stockportfolio-"+pattern
                  db_name = GlueDatabaseName+pattern
                  print("GlueDatabaseName: " + str(db_name))
                  ret_dict["CrawlerName"]=crawler_name
                  job_role = role_arn 
                  #.rpartition("/")[-1]
                  
                  try:
                      glue_client = boto3.client('glue')
                      s3_source = "s3://"+ bucket_name +"/data/"
                      response = glue_client.create_crawler(
                          Name=crawler_name,
                          Role=job_role,
                          DatabaseName=db_name,
                          Targets={
                              "S3Targets": [
                                  {
                                      "Path": s3_source
                                  }
                              ]
                          },
                          SchemaChangePolicy={
                              'UpdateBehavior': 'UPDATE_IN_DATABASE',
                              'DeleteBehavior': 'DEPRECATE_IN_DATABASE'
                          },
                          Description='Crawler for S3',
                          Configuration='{"Version":1.0,"CrawlerOutput":{"Partitions":{"AddOrUpdateBehavior":"InheritFromTable"},"Tables":{"AddOrUpdateBehavior":"MergeNewColumns"}}}'
                      )
                  except Exception as e:
                      print("Failed provisioning glue crawler "+str(e))
                  
                  try:
                    print("waiting: " + str(60))
                    time.sleep(60)
                    #wait_for_crawler_creation(crawler_name)
                    #print('Crawler', crawler_name, 'is created.')
                    
                    response = run_glue_crawler(crawler_name)
                    print('Crawler', crawler_name, 'is running:', response['ResponseMetadata']['HTTPStatusCode'])
                  
                  except Exception as e:
                      print("Failed running glue crawler "+str(e))

                  return ret_dict

              def handle_delete(bucket_name,GlueDatabaseName, pattern,role_arn,region_name, account_id,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr):
                  dict_return={}
                  dict_return["Data"]="test_delete"
                  empty_bucket(bucket_name,region_name,account_id)  
                  delete_crawler(pattern)
                  drop_database(GlueDatabaseName, pattern)

                  return dict_return  

              def handle_create(bucket_name,GlueDatabaseName, pattern,role_arn,region_name, account_id,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr,kb_role_arn,lambda_arn, kb_collection_url):
                  print('start handle create')
                  dict_return={}
                  dict_return=provision_s3_dirs(bucket_name,region_name, account_id,dict_return)
                  dict_return=deploy_assets(bucket_name,region_name, account_id,role_arn, dict_return)
                  dict_return=create_glue_database(bucket_name,GlueDatabaseName,pattern,dict_return)
                  dict_return=deploy_and_run_glue_crawler(role_arn, pattern, GlueDatabaseName, bucket_name, dict_return)

                  return dict_return

              def lambda_handler(event, context):
                  response_ = cfnresponse.SUCCESS
                  print(str(event))
                  return_dict={}
                  physical_resourceId = ''.join(random.choices(string.ascii_lowercase +string.digits, k=7))
                  
                  
                  try:
                      account_id = context.invoked_function_arn.split(":")[4]
                      role_arn = str(os.environ['ROLE_ARN'])
                      kb_role_arn = str(os.environ['KB_ROLE_ARN'])
                      kb_collection_url = str(os.environ['KB_COLLECTION_URL'])
                      lambda_arn = str(os.environ['LAMBDA_ARN'])
                      region_name = str(os.environ['AWS_REGION'])
                      bucket_arg = str(os.environ['BUCKET'])
                      random_string_arg = str(os.environ['RANDOM_STRING'])
                      agent_model = str(os.environ['AGENT_MODEL'])
                      agent_name = str(os.environ['AGENT_NAME'])
                      agent_instruction = str(os.environ['AGENT_INSTRUCTION'])
                      agent_actiongroupname = str(os.environ['AGENT_ACTION_GROUP'])
                      agent_kb_name = str(os.environ['AGENT_KB'])
                      agent_kb_descr = str(os.environ['AGENT_KB_DESCR'])

                      GlueDatabaseName = agent_name

                      request_type = str(event.get("RequestType",""))
                      print('picked up event: '+ str(request_type))
                      if request_type=='Create':
                          return_dict = handle_create(bucket_arg, GlueDatabaseName, random_string_arg,role_arn,region_name, account_id,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr,kb_role_arn,lambda_arn,kb_collection_url)
                      elif request_type =='Delete':
                          return_dict = handle_delete(bucket_arg, GlueDatabaseName, random_string_arg,role_arn,region_name, account_id,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr)
                      else:
                          return_dict = {}
                          return_dict["Data"] = "testupdate"
                  except Exception as e:
                    return_dict['Data'] = str(e)
                    response_ = cfnresponse.FAILED
                  cfnresponse.send(event,context,response_,return_dict,physical_resourceId)
        
        Role: !GetAtt BedrockAgentToolsFunctionRole.Arn
        Environment:
            Variables:
                BUCKET: !Ref DataBucket
                RANDOM_STRING: !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]
                ROLE_ARN: !GetAtt BedrockAgentToolsFunctionRole.Arn
                KB_ROLE_ARN: !GetAtt BedrockKBRole.Arn
                KB_COLLECTION_URL: !GetAtt TestCollection.CollectionEndpoint
                LAMBDA_ARN: !GetAtt BedrockAgentToolsLambdaFunction.Arn
                AGENT_MODEL: !Ref AgentFoundationModel
                AGENT_NAME: !Ref AgentName
                AGENT_INSTRUCTION: !Ref AgentInstruction
                AGENT_ACTION_GROUP: !Ref AgentActionGroupName
                AGENT_KB: !Ref KnowledgeBaseName
                AGENT_KB_DESCR: !Ref KnowledgeBaseDescription
        Runtime: python3.9
        Timeout: '900'
        MemorySize: '128'
        EphemeralStorage: 
            Size: 512 
  
  EnableSetupLambda:
    DependsOn:
      - LambdaSetupFunction
      - BedrockAgentToolsFunctionRole
    Type: Custom::EnableLambda
    Version: '1.0'
    Properties:
        ServiceToken: !GetAtt LambdaSetupFunction.Arn

  AOSlayer:
    Type: AWS::Lambda::LayerVersion
    DependsOn: 
      - EnableSetupLambda
    Properties:
      LayerName: OpenSearchLayer
      Description: opensearch-py layer
      Content:
        S3Bucket: !Ref DataBucket
        S3Key: 'code/opensearch-lib.zip'
      CompatibleRuntimes:
        - python3.9
        - python3.10
        - python3.11

  BedrockSetupFunction:
    Type: AWS::Lambda::Function
    DependsOn: 
      - BedrockAgentToolsFunctionRole
      - DataBucket
      - AOSlayer
    Properties:
        Description: ""
        FunctionName: !Join ['_', ['setup_bedrock_lambda', !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]]]
        VpcConfig:
          SecurityGroupIds: 
            - !Ref opsSecurityGroup
          SubnetIds:
            - !Ref PrivateSubnetApp1
        Layers:
          - !Ref AOSlayer
          - 'arn:aws:lambda:us-east-1:770693421928:layer:Klayers-p39-boto3:23'
          - 'arn:aws:lambda:us-east-1:026459568683:layer:requests-aws4auth:1'
        Handler: "index.lambda_handler"
        Code: 
            ZipFile : |
              import json
              import boto3
              import os
              import cfnresponse
              import string
              import random
              import urllib3
              import shutil
              import time
              from botocore.exceptions import ClientError
              from opensearchpy import OpenSearch, RequestsHttpConnection
              from requests_aws4auth import AWS4Auth

              import subprocess

              def run_shell(cmd):
                  process = subprocess.Popen(
                      cmd.split(' '),
                      encoding='utf-8',
                      stdin=subprocess.PIPE,
                      stdout=subprocess.PIPE,
                  )

                  while(True):
                      returncode = process.poll()
                      if returncode is None:
                          # You describe what is going on.
                          # You can describe the process every time the time elapses as needed.
                          # print("running process")
                          time.sleep(0.01)
                          data = process.stdout
                          if data:
                              # If there is any response, describe it here.
                              # You need to use readline () or readlines () properly, depending on how the process responds.
                              msg_line = data.readline()
                              print(msg_line, end="")
                          err = process.stderr
                          if err:
                              # If there is any error response, describe it here.
                              msg_line = err.readline()
                              print(msg_line, end="")
                      else:
                          break

              def delete_bedrock_kb(role_arn,bucket_name,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr, dict_return):

                  agent_resource_role_arn = role_arn
                  knowledge_base_resource_role_arn = role_arn
                  open_api_s3_url = 's3://' + bucket_name + '/code/agent_aws_openapi.json'

                  knowledge_base_bucket = bucket_name
                  foundation_model = agent_model
                  instruction = agent_instruction
                  description = agent_instruction
                  knowledge_base_name = agent_kb_name
                  knowledge_base_description = agent_kb_descr
                  action_group_name = agent_actiongroupname
                  data_source_name =  agent_kb_name

                  bedrock_agent_client = boto3.client('bedrock-agent')
                  paginator = bedrock_agent_client.get_paginator('list_agents')

                  response_iterator = paginator.paginate()
                  agent_id = ""
                  agent_version = ""
                  data_source_id = ""
                  action_group_id = ""

                  for page in response_iterator:
                      for agent in page['agentSummaries']:
                          if agent['agentName'] == agent_name:
                              agent_id = agent['agentId']
                              
                  if agent_id != "":
                      agent_versions = bedrock_agent_client.list_agent_versions(agentId = agent_id)
                      agent_version = agent_versions['agentVersionSummaries'][0]['agentVersion']  

                  if agent_id != "" and agent_version != "":
                      agent_list_response = bedrock_agent_client.list_agent_action_groups(
                          agentId=agent_id,
                          agentVersion=agent_version,
                      )
                      
                      for agent in agent_list_response['actionGroupSummaries']:
                          if agent['actionGroupName'] == action_group_name:
                              action_group_id = agent['actionGroupId']
                          
                  paginator = bedrock_agent_client.get_paginator('list_knowledge_bases')

                  response_iterator = paginator.paginate()

                  for page in response_iterator:
                      for kb in page['knowledgeBaseSummaries']:
                          if kb['name'] == knowledge_base_name:
                              knowledge_base_id = kb['knowledgeBaseId']

                  if knowledge_base_id != "":
                      data_source_list_response = bedrock_agent_client.list_data_sources(
                          knowledgeBaseId=knowledge_base_id
                      )

                      for data_source in data_source_list_response['dataSourceSummaries']:
                          if data_source['name'] == data_source_name:
                              data_source_id = data_source['dataSourceId']



                  print(f"Agent Id: {agent_id}")
                  print(f"Agent Version: {agent_version}")
                  print(f"Action Group Id: {action_group_id}")
                  print(f"Knowledgebase Id: {knowledge_base_id}")
                  print(f"Datasource Id: {data_source_id}")

                  bucket= open_api_s3_url[5:open_api_s3_url[5:].find('/')+5]
                  key = open_api_s3_url[open_api_s3_url[5:].find('/')+6:]

                  action_group_config = {
                      "agentId": agent_id,
                      "agentVersion": agent_version,
                      "actionGroupId": action_group_id,
                      "actionGroupName": action_group_name,
                      "description": "Agent Finance is an automated, AI-powered agent that helps customers with financial investments by retrieving financial data about companies from the internet and checking a users existing portfolio.",
                      "actionGroupExecutor" : {
                          "lambda": lambda_arn
                      },
                      "apiSchema": {
                          "s3": {
                              "s3BucketName": bucket,
                              "s3ObjectKey": key
                          }
                      },
                      "actionGroupState": "DISABLED"
                  }

                  try:
                      action_group_result = bedrock_agent_client.update_agent_action_group(**action_group_config)
                      action_group_result = bedrock_agent_client.delete_agent_action_group(
                          agentId=agent_id,
                          agentVersion=agent_version,
                          actionGroupId=action_group_id
                      )
                      print("Action Group Deleted")
                  except:
                      print("Unable to Delete Action Group.")


                  #response = bedrock_agent_client.disassociate_agent_knowledge_base(
                  #    agentId=agent_id,
                  #    agentVersion=agent_version,
                  #    knowledgeBaseId=knowledge_base_id
                  #)

                  try:
                      response = bedrock_agent_client.delete_data_source(
                          knowledgeBaseId=knowledge_base_id,
                          dataSourceId=data_source_id
                      )
                      print("Data Source Deleted")
                  except:
                      print("Unable to Delete Data Source.")

                  try:
                      response = bedrock_agent_client.delete_knowledge_base(
                          knowledgeBaseId=knowledge_base_id
                      )
                      print("Knowledge Base Deleted")
                  except:
                      print("Unable to Delete Knowledge Base.")

                  try:
                      response = bedrock_agent_client.delete_agent(
                          agentId=agent_id
                      )
                      print("Agent Deleted")
                  except:
                      print("Unable to Delete Agent.")
                      
                  run_shell(f"aws s3 rm --recursive s3://{knowledge_base_bucket.split(':')[5]}")
                  run_shell(f"aws s3 rm --recursive s3://{knowledge_base_bucket}")


                  return "bedrock agent and kb {} deleted ".format(agent_name)

              def create_bedrock_kb(role_arn,bucket_name,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr,kb_role_arn, lambda_arn, kb_collection_url, collection_arn, dict_return):

                  agent_resource_role_arn = role_arn
                  knowledge_base_resource_role_arn = kb_role_arn
                  open_api_s3_url = 's3://' + bucket_name + '/code/agent_aws_openapi.json'
                  
                  agent_id = ""
                  agent_version = ""
                  knowledge_base_id = ""
                  data_source_id = ""
                  action_group_id = ""

                  knowledge_base_bucket_arn = 'arn:aws:s3:::' + bucket_name
                  foundation_model = agent_model
                  instruction = agent_instruction
                  description = agent_instruction
                  knowledge_base_name = agent_kb_name
                  knowledge_base_description = agent_kb_descr
                  action_group_name = agent_actiongroupname
                  data_source_name =  agent_kb_name

                  agent_config = {
                      "agentName": agent_name,
                      "instruction": instruction,
                      "foundationModel": foundation_model,
                      "description": description,
                      "idleSessionTTLInSeconds": 60,
                      "agentResourceRoleArn": agent_resource_role_arn
                  }

                  response = {}
                  current_session = boto3.session.Session()
                  region = current_session.region_name
                  print(f"The current region is {region}")
                  bedrock_agent_client = boto3.client('bedrock-agent', region_name = region)

                  if agent_id == "":
                      response = bedrock_agent_client.create_agent(**agent_config)
                      agent_id = response['agent']['agentId']
                      
                  agent_versions = bedrock_agent_client.list_agent_versions(agentId = agent_id)

                  agent_version = agent_versions['agentVersionSummaries'][0]['agentVersion']  
                  print(f"Agent Version: {agent_version}")
                  time.sleep(5)
                  bucket= bucket_name
                  print('bucketname: ' + str(bucket))
                  key = open_api_s3_url[open_api_s3_url[5:].find('/')+6:]
                  print('bucket key: ' + str(key))
                  print('action_group_name: ' + str(action_group_name))
                  print('lambda_arn: ' + str(lambda_arn))
                  
                  action_group_config = {
                      "agentId": agent_id,
                      "agentVersion": agent_version,
                      "actionGroupName": action_group_name,
                      "description": "Agent Finance is an automated, AI-powered agent that helps customers with financial investments by retrieving financial data about companies from the internet and checking a users existing portfolio.",
                      "actionGroupExecutor" : {
                          "lambda": lambda_arn
                      },
                      "apiSchema": {
                          "s3": {
                              "s3BucketName": bucket,
                              "s3ObjectKey": key
                          }
                      },
                      "actionGroupState": "ENABLED"
                  }
                  
                  action_group_response = bedrock_agent_client.create_agent_action_group(**action_group_config)
                  print(str(action_group_response))
                  action_group_id = action_group_response['agentActionGroup']['actionGroupId']
                  
                  response = bedrock_agent_client.prepare_agent(
                      agentId=agent_id
                  )

                  # set up Opensearch Serverless collection for KB
                  opensearch_hostname = kb_collection_url.replace("https://", "")

                  service = 'aoss'
                  credentials = current_session.get_credentials()

                  awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service,
                  session_token=credentials.token)

                  # Create an OpenSearch client
                  client = OpenSearch(
                      hosts = [{'host': opensearch_hostname, 'port': 443}],
                      http_auth = awsauth,
                      timeout = 300,
                      use_ssl = True,
                      verify_certs = True,
                      connection_class = RequestsHttpConnection
                  )
                  index_name = "bedrock-kb-demo"
                  vector_field = "kb_vector"
                  text_field = "kb_text"
                  bedrock_metadata_field = "bedrock"
                  vector_size = 1536

                  index_found = False
                  try:
                      client.indices.get(index=index_name)
                      index_found = True
                  except:
                      print("Index does not exist, create the index")

                  #create a new index
                  if index_found == False:
                      index_body = {
                          "settings": {
                              "index.knn": True
                        },
                        'mappings': {
                          'properties': {
                            f"{vector_field}": { "type": "knn_vector", "dimension": vector_size, "method": {"engine": "nmslib", "space_type": "cosinesimil", "name": "hnsw", "parameters": {}   } },
                            f"{text_field}": { "type": "text" },
                            f"{bedrock_metadata_field}": { "type": "text", "index": False }
                          }
                        }
                      }

                      client.indices.create(
                        index=index_name, 
                        body=index_body
                      )
                      # wait 30 seconds for index creation to complete
                      time.sleep(30)

                  client.indices.get(index=index_name)

                  # create knowledge base
                  knowledge_base_config = {
                      "name": knowledge_base_name,
                      "description": knowledge_base_description,
                      "roleArn":knowledge_base_resource_role_arn,
                      "knowledgeBaseConfiguration": {
                          "type": 'VECTOR',
                          "vectorKnowledgeBaseConfiguration": {
                              "embeddingModelArn": "arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1"
                          }
                      },
                      "storageConfiguration": {
                          "type": 'OPENSEARCH_SERVERLESS',
                          "opensearchServerlessConfiguration": {
                              "collectionArn": collection_arn,
                              "vectorIndexName": index_name,
                              "fieldMapping": {
                                  "vectorField": vector_field,
                                  "textField": text_field,
                                  "metadataField": bedrock_metadata_field
                              }
                          }
                      }
                  }

                  if knowledge_base_id == "":
                      response = bedrock_agent_client.create_knowledge_base(**knowledge_base_config)
                      knowledge_base_id = response['knowledgeBase']['knowledgeBaseId']                      

                  data_source_id = ""
                  max_token_chunk = 150
                  overlap = 5

                  response = bedrock_agent_client.list_data_sources(
                      knowledgeBaseId=knowledge_base_id
                  )
                  for data_source in response['dataSourceSummaries']:
                      if data_source['knowledgeBaseId'] == knowledge_base_id:
                          data_source_id = data_source['dataSourceId']

                  # configure data_source
                  data_source_config = {
                      "knowledgeBaseId": knowledge_base_id,
                      "name": data_source_name,
                      "description": data_source_name,
                      "dataSourceConfiguration": {
                          "type": 'S3',
                          "s3Configuration": {
                              "bucketArn": knowledge_base_bucket_arn
                          }
                      },
                      "vectorIngestionConfiguration": {
                          "chunkingConfiguration": {
                              "chunkingStrategy": "FIXED_SIZE",
                              "fixedSizeChunkingConfiguration": {
                                  "maxTokens": max_token_chunk,
                                  "overlapPercentage": overlap
                              }
                          }
                      }
                  }

                  ds_response = bedrock_agent_client.create_data_source(**data_source_config)
                  data_source_id = ds_response['dataSource']['dataSourceId']
                  
                  response = bedrock_agent_client.list_data_sources( knowledgeBaseId=knowledge_base_id)
                  for data_source in response['dataSourceSummaries']:
                      if data_source['knowledgeBaseId'] == knowledge_base_id:
                          data_source_id = data_source['dataSourceId']

                  print(f"data_source_id: {data_source_id}")
                  
                  ingestion_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId=knowledge_base_id,dataSourceId=data_source_id)
                  print(f"ingestion response: {ingestion_response}")
                  
                  ingestion_job = ingestion_response['ingestionJob']['ingestionJobId']
                  print(f"ingestionJobId: {ingestion_job}")
                  status = 'IN_PROGRESS'
                  response = {}
                  
                  while status == 'IN_PROGRESS':
                      response = bedrock_agent_client.get_ingestion_job(
                          knowledgeBaseId=knowledge_base_id,
                          dataSourceId=data_source_id,
                          ingestionJobId=ingestion_job
                      )
                      
                      status = response['ingestionJob']['status']
                      time.sleep(5)
                  
                  print(response['ingestionJob']['statistics'])
                  
                  response = bedrock_agent_client.associate_agent_knowledge_base(
                      agentId=agent_id,
                      agentVersion=agent_version,
                      knowledgeBaseId=knowledge_base_id,
                      description='"This knowledge base provides access to financial data"',
                      knowledgeBaseState='ENABLED'
                  )
                  print(f"associate_agent_knowledge_base response: {response}")
                  print("Deploy Complete")


                  dict_return["Bedrock"]="deployment of Bedrock Agent and KB complete"
                  return dict_return

              def handle_delete(bucket_name,GlueDatabaseName, pattern,role_arn,region_name, account_id,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr):
                  dict_return={}
                  dict_return["Data"]="test_delete"
                  delete_bedrock_kb(role_arn, bucket_name,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr, dict_return)

                  return dict_return  

              def handle_create(bucket_name,GlueDatabaseName, pattern,role_arn,region_name, account_id,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr,kb_role_arn,lambda_arn, kb_collection_url,collection_arn):
                  print('start handle create')
                  dict_return={}

                  dict_return=create_bedrock_kb(role_arn, bucket_name,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr,kb_role_arn, lambda_arn, kb_collection_url,collection_arn, dict_return)

                  return dict_return

              def lambda_handler(event, context):
                  response_ = cfnresponse.SUCCESS
                  print(str(event))
                  return_dict={}
                  physical_resourceId = ''.join(random.choices(string.ascii_lowercase +string.digits, k=7))
                  
                  
                  try:
                      account_id = context.invoked_function_arn.split(":")[4]
                      role_arn = str(os.environ['ROLE_ARN'])
                      kb_role_arn = str(os.environ['KB_ROLE_ARN'])
                      kb_collection_url = str(os.environ['KB_COLLECTION_URL'])
                      lambda_arn = str(os.environ['LAMBDA_ARN'])
                      region_name = str(os.environ['AWS_REGION'])
                      bucket_arg = str(os.environ['BUCKET'])
                      random_string_arg = str(os.environ['RANDOM_STRING'])
                      agent_model = str(os.environ['AGENT_MODEL'])
                      agent_name = str(os.environ['AGENT_NAME'])
                      agent_instruction = str(os.environ['AGENT_INSTRUCTION'])
                      agent_actiongroupname = str(os.environ['AGENT_ACTION_GROUP'])
                      collection_arn = str(os.environ['KB_COLLECTION_ARN'])
                      agent_kb_name = str(os.environ['AGENT_KB'])
                      agent_kb_descr = str(os.environ['AGENT_KB_DESCR'])
                      ec2_instance_id = str(os.environ['EC2_TEMP_INSTANCE'])
                      ecr_repo = str(os.environ['ECR_REPO'])
                      GlueDatabaseName = agent_name

                      request_type = str(event.get("RequestType",""))
                      print('picked up event: '+ str(request_type))
                      if request_type=='Create':
                          return_dict = handle_create(bucket_arg, GlueDatabaseName, random_string_arg,role_arn,region_name, account_id,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr,kb_role_arn,lambda_arn,kb_collection_url,collection_arn)
                          # delete temp EC2 instance
                          ec2client = boto3.client('ec2', region_name=region_name)
                          print(f"stopping ec2 instance id: {ec2_instance_id}")
                          response = ec2client.stop_instances(InstanceIds=[ec2_instance_id])
                          print(f"response from stopping EC2 instance: {response}")
                          time.sleep(20)
                          print(f"terminating ec2 instance id: {ec2_instance_id}")
                          response = ec2client.terminate_instances(InstanceIds=[ec2_instance_id])
                          print(f"response from terminating EC2 instance: {response}")

                      elif request_type =='Delete':
                          return_dict = handle_delete(bucket_arg, GlueDatabaseName, random_string_arg,role_arn,region_name, account_id,agent_model,agent_name,agent_instruction,agent_actiongroupname,agent_kb_name,agent_kb_descr)
                          # delete temp EC2 instance
                          ec2client = boto3.client('ec2', region_name=region_name)
                          print(f"stopping ec2 instance id: {ec2_instance_id}")
                          response = ec2client.stop_instances(InstanceIds=[ec2_instance_id])
                          print(f"response from stopping EC2 instance: {response}")
                          time.sleep(20)
                          print(f"terminating ec2 instance id: {ec2_instance_id}")
                          response = ec2client.terminate_instances(InstanceIds=[ec2_instance_id])
                          print(f"response from terminating EC2 instance: {response}")
                          ecrclient = boto3.client('ecr')
                          response = ecrclient.delete_repository(
                              registryId=account_id,
                              repositoryName=ecr_repo,
                              force=True
                          )
                      else:
                          return_dict = {}
                          return_dict["Data"] = "testupdate"
                  except Exception as e:
                    return_dict['Data'] = str(e)
                    response_ = cfnresponse.FAILED
                  cfnresponse.send(event,context,response_,return_dict,physical_resourceId)
        
        Role: !GetAtt BedrockAgentToolsFunctionRole.Arn
        Environment:
            Variables:
                BUCKET: !Ref DataBucket
                RANDOM_STRING: !Select [4, !Split ['-', !Select [2, !Split ['/', !Ref AWS::StackId]]]]
                ROLE_ARN: !GetAtt BedrockAgentToolsFunctionRole.Arn
                KB_ROLE_ARN: !GetAtt BedrockKBRole.Arn
                KB_COLLECTION_ARN: !GetAtt TestCollection.Arn
                KB_COLLECTION_URL: !GetAtt TestCollection.CollectionEndpoint
                LAMBDA_ARN: !GetAtt BedrockAgentToolsLambdaFunction.Arn
                AGENT_MODEL: !Ref AgentFoundationModel
                AGENT_NAME: !Ref AgentName
                AGENT_INSTRUCTION: !Ref AgentInstruction
                AGENT_ACTION_GROUP: !Ref AgentActionGroupName
                AGENT_KB: !Ref KnowledgeBaseName
                AGENT_KB_DESCR: !Ref KnowledgeBaseDescription
                EC2_TEMP_INSTANCE: !Ref DockerPushInstance
                ECR_REPO: !Ref ECRRepoName
        Runtime: python3.9
        Timeout: '900'
        MemorySize: '128'
        EphemeralStorage: 
            Size: 512

  EnableBedrockLambda:
    DependsOn:
      - BedrockSetupFunction
      - BedrockAgentToolsFunctionRole
    Type: Custom::EnableLambda
    Version: '1.0'
    Properties:
        ServiceToken: !GetAtt BedrockSetupFunction.Arn
Outputs:
  StackName:
    Description: This is the stack name.
    Value: !Ref 'AWS::StackName'
    Export:
      Name: !Sub '${AWS::StackName}-StackName'
  VPCCIDRBlock:
    Description: This is the VPC CIDR Block.
    Value: !Join 
      - ''
      - - !Ref CIDRPrefix
        - .0.0/21
    Export:
      Name: !Sub '${AWS::StackName}-VPCCIDRBlock'
  VPCCIDRPrefix:
    Description: This is the VPC CIDR Prefix For Offsetting in your chained stacks.
    Value: !Ref CIDRPrefix
    Export:
      Name: !Sub '${AWS::StackName}-VPCCIDRPrefix'
  EnvTag:
    Description: This is the environment tag to use for other stacks to inherit.
    Value: !Ref EnvironmentTag
    Export:
      Name: !Sub '${AWS::StackName}-EnvTag'
  PublicSubnet0:
    Description: Public subnet 0 for Load Balancer
    Value: !Ref PublicSubnet0
    Export:
      Name: !Sub '${AWS::StackName}-PublicSubnet0'
  PublicSubnet1:
    Description: Public subnet 1 for Load Balancer
    Value: !Ref PublicSubnet1
    Export:
      Name: !Sub '${AWS::StackName}-PublicSubnet1'
  PublicSubnet2:
    Description: Public subnet 2 for Load Balancer
    Value: !Ref PublicSubnet2
    Export:
      Name: !Sub '${AWS::StackName}-PublicSubnet2'
  PrivateSubnetApp0:
    Description: Private subnet 0 for Application
    Value: !Ref PrivateSubnetApp0
    Export:
      Name: !Sub '${AWS::StackName}-PrivateSubnetApp0'
  PrivateSubnetApp1:
    Description: Private subnet 1 for Application
    Value: !Ref PrivateSubnetApp1
    Export:
      Name: !Sub '${AWS::StackName}-PrivateSubnetApp1'
  PrivateSubnetApp2:
    Description: Private subnet 2 for Application
    Value: !Ref PrivateSubnetApp2
    Export:
      Name: !Sub '${AWS::StackName}-PrivateSubnetApp2'
  ApplicationCIDRRange:
    Description: This is the Application CIDR Range.
    Value: !Join 
      - ''
      - - !Ref CIDRPrefix
        - .4.0/22
    Export:
      Name: !Sub '${AWS::StackName}-ApplicationCIDRRange'
  PublicCIDRRange:
    Description: This is the Application CIDR Range.
    Value: !Join 
      - ''
      - - !Ref CIDRPrefix
        - .0.0/22
    Export:
      Name: !Sub '${AWS::StackName}-PublicCIDRRange'
  PublicRoutingTable:
    Description: Public Route Table
    Value: !Ref PublicRoutingTable
    Export:
      Name: !Sub '${AWS::StackName}-PublicRoutingTable'
  PrivateRoutingTable:
    Description: Private Route Table
    Value: !Ref PrivateRoutingTable
    Export:
      Name: !Sub '${AWS::StackName}-PrivateRoutingTable'
  NATPublicIP:
    Description: >-
      This is the NAT Public IP address for external whitelisting of external
      repos and packages.
    Value: !Ref NATGatewayIPAddress
    Export:
      Name: !Sub '${AWS::StackName}-NATPublicIP'
  VPCID:
    Description: This is the VPC you have created
    Value: !Ref VPC
    Export:
      Name: !Sub '${AWS::StackName}-VPCID'
  OpenSearchArn:
    Value:
      'Fn::GetAtt':
        - OpenSearchServiceDomain
        - Arn
  OpenSearchDomainEndpoint:
    Value:
      'Fn::GetAtt':
        - OpenSearchServiceDomain
        - DomainEndpoint
